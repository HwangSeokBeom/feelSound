////
////  Untitled.swift
////  feelsound
////
////  Created by Hwangseokbeom on 5/23/25.
////
//
//import AVFoundation
//import CoreML
//import AudioKitEX
//
//class AudioEmotionAnalyzer {
//    private let engine = AVAudioEngine()
//    private let inputBus: AVAudioNodeBus = 0
//    private let sampleRate: Double = 16000
//    private let bufferSize: AVAudioFrameCount = 1024  // ÏïΩ 64ms Î∂ÑÎüâ
//    private var lastEmotionTime: TimeInterval = 0
//    private let emotionCooldown: TimeInterval = 5.0
//
//    weak var delegate: ArcticFoxScene?  // üí° SpriteKit SceneÍ≥º Ïó∞Îèô
//
//    func start() {
//        do {
//            let session = AVAudioSession.sharedInstance()
//
//            // üé§ ÎßàÏù¥ÌÅ¨ ÏÇ¨Ïö© ÏÑ§Ï†ï (playAndRecordÎ°ú Ìï¥Ïïº ÎßàÏù¥ÌÅ¨ ÌóàÏö©Îê®)
//            try session.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker])
//
//            // ÏãúÏä§ÌÖúÏù¥ Ìò∏Ìôò Í∞ÄÎä•Ìïú ÏÉòÌîåÎ†àÏù¥Ìä∏ ÏÑ§Ï†ï
//            try session.setPreferredSampleRate(44100)
//            try session.setActive(true)
//
//            let inputNode = engine.inputNode
//            let format = inputNode.outputFormat(forBus: inputBus) // üí° ÏãúÏä§ÌÖúÏù¥ Î∞òÌôòÌïòÎäî Í∏∞Î≥∏ Ìè¨Îß∑ ÏÇ¨Ïö©
//
//            // üîÅ Ìè¨Îß∑ÏùÑ Í∞ïÏ†úÎ°ú ÏßÄÏ†ïÌïòÏßÄ ÎßàÏÑ∏Ïöî! format: nil Ïù¥ Îçî ÏïàÏ†Ñ
//            inputNode.installTap(onBus: inputBus, bufferSize: bufferSize, format: nil) { buffer, time in
//                self.processAudioBuffer(buffer, format: format)
//            }
//
//            try engine.start()
//            print("üé§ ÎßàÏù¥ÌÅ¨ ÏûÖÎ†• ÏãúÏûëÎê®")
//
//        } catch {
//            print("‚ùå AVAudioEngine ÏãúÏûë Ïã§Ìå®: \(error.localizedDescription)")
//        }
//    }
//
//    func stop() {
//        engine.inputNode.removeTap(onBus: inputBus)
//        engine.stop()
//    }
//
//    func processAudioBuffer(_ buffer: AVAudioPCMBuffer, format: AVAudioFormat) {
//        let now = CACurrentMediaTime()
//        guard now - lastEmotionTime > emotionCooldown else { return }  // Ïø®ÌÉÄÏûÑ Ï≤¥ÌÅ¨
//
//        guard let channelData = buffer.floatChannelData?[0] else { return }
//        let frameLength = Int(buffer.frameLength)
//        let audioData = Array(UnsafeBufferPointer(start: channelData, count: frameLength))
//
//        let mfccVector = extractMFCC(from: audioData, sampleRate: format.sampleRate)
//
//        if let emotion = predictEmotion(from: mfccVector) {
//            lastEmotionTime = now  // Ïø®ÌÉÄÏûÑ Í∞±Ïã†
//            print("üîÆ Í∞êÏ†ï Ï∂îÎ°† Í≤∞Í≥º: \(emotion)")
//            DispatchQueue.main.async {
//                self.delegate?.performAction(for: emotion)
//            }
//        }
//    }
//
//    func predictEmotion(from mfcc: [Float]) -> String? {
//        guard mfcc.count == 26 else { return nil }
//
//        guard let mlModel = try? EmotionVectorClassifier(configuration: .init()),
//              let inputArray = try? MLMultiArray(shape: [1, 26], dataType: .float32) else {
//            return nil
//        }
//
//        for (i, value) in mfcc.enumerated() {
//            inputArray[i] = NSNumber(value: value)
//        }
//
//        guard let result = try? mlModel.prediction(input_1: inputArray) else {
//            return nil
//        }
//
//        return result.classLabel
//    }
//
//    func extractMFCC(from audio: [Float], sampleRate: Double) -> [Float] {
//        let frameCount = 512
//        let hopCount = 256
//
//        guard let mfccExtractor = MFCC(
//            numberOfCoefficients: 26,
//            windowSize: frameCount,
//            hopSize: hopCount,
//            sampleRate: Double(Float(sampleRate))
//        ) else {
//            print("‚ùå MFCC ÏÉùÏÑ± Ïã§Ìå®")
//            return []
//        }
//
//        let result = mfccExtractor.process(audio)
//
//        let averaged = result.reduce(into: Array(repeating: 0.0 as Float, count: 26)) { sum, vec in
//            for i in 0..<26 {
//                sum[i] += vec[i]
//            }
//        }.map { $0 / Float(result.count) }
//
//        return averaged
//    }
//    
//    func isVoiceDetected(audioData: [Float]) -> Bool {
//        // RMS ÏóêÎÑàÏßÄ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (ÏóêÎÑàÏßÄÍ∞Ä ÎÇÆÏúºÎ©¥ Î¨¥ÏÑ±ÏúºÎ°ú ÌåêÎã®)
//        let energy = audioData.reduce(0) { $0 + $1 * $1 } / Float(audioData.count)
//        return energy > 0.005  // ‚úÖ Ïã§ÌóòÏ†ÅÏúºÎ°ú ÌäúÎãù ÌïÑÏöî
//    }
//    
//    func isVoiceDetected(audioData: [Float], sampleRate: Double) -> Bool {
//        let energy = audioData.reduce(0) { $0 + $1 * $1 } / Float(audioData.count)
//
//        // Zero-Crossing Rate Í≥ÑÏÇ∞
//        var zeroCrossings = 0
//        for i in 1..<audioData.count {
//            if (audioData[i - 1] >= 0 && audioData[i] < 0) ||
//               (audioData[i - 1] < 0 && audioData[i] >= 0) {
//                zeroCrossings += 1
//            }
//        }
//        let zcr = Double(zeroCrossings) / Double(audioData.count)
//
//        // üí° Í∏∞Ï§ÄÍ∞íÏùÄ ÌôòÍ≤ΩÏóê Îî∞Îùº Ï°∞Ï†ï Í∞ÄÎä•
//        let isEnergyValid = energy > 0.001   // Ïã§ÌóòÏ†ÅÏúºÎ°ú ÌäúÎãù ÌïÑÏöî
//        let isZCRValid = zcr > 0.01 && zcr < 0.2 // ÎßêÏÜåÎ¶¨Ïóê Ï†ÅÏ†àÌïú Î≤îÏúÑ
//
//        return isEnergyValid && isZCRValid
//    }
//}
