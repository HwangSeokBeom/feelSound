//
//  SpeechRecognizer.swift
//  feelsound
//
//  Created by Hwangseokbeom on 5/27/25.
//

import Foundation
import AVFoundation
import Speech

class SpeechRecognizer: NSObject, ObservableObject {
    private let engine = AVAudioEngine()
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ko-KR"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let emotionAnalyzer = EmotionAnalyzer()

    weak var foxScene: ArcticFoxScene?

    @Published var recognizedText = ""
    @Published var isListening = false

    private var lastVoiceDetectedTime: TimeInterval = CACurrentMediaTime()
    private let silenceTimeout: TimeInterval = 5.0
    private var silenceCheckTimer: Timer?

    // MARK: - ê¶Œí•œ ìš”ì²­ + ë…¹ìŒ ì‹œìž‘
    func requestPermissionAndStart() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                if status == .authorized {
                    print("âœ… ìŒì„± ì¸ì‹ ê¶Œí•œ í—ˆìš©ë¨")
                    self.requestMicPermissionAndStart()
                } else {
                    print("âŒ ìŒì„± ì¸ì‹ ê¶Œí•œ ê±°ë¶€ë¨")
                }
            }
        }
    }

    private func requestMicPermissionAndStart() {
        AVAudioSession.sharedInstance().requestRecordPermission { granted in
            DispatchQueue.main.async {
                if granted {
                    print("ðŸŽ¤ ë§ˆì´í¬ ê¶Œí•œ í—ˆìš©ë¨")
                    self.startRecording()
                } else {
                    print("ðŸš« ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ë¨")
                }
            }
        }
    }

    // MARK: - ë…¹ìŒ ì‹œìž‘
    func startRecording() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            print("âŒ SFSpeechRecognizer ì‚¬ìš© ë¶ˆê°€ ë˜ëŠ” ì¸ì‹ê¸° ì—†ìŒ")
            return
        }

        if engine.isRunning {
            print("âš ï¸ AVAudioEngine ì´ë¯¸ ì‹¤í–‰ ì¤‘")
            return
        }

        stopRecording()
        recognizedText = ""

        // ì„¸ì…˜ ì„¤ì •
        do {
            let session = AVAudioSession.sharedInstance()
            try session.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .mixWithOthers])
            try session.setActive(true, options: .notifyOthersOnDeactivation)
            print("âœ… AVAudioSession ì„¤ì • ì™„ë£Œ")
        } catch {
            print("âŒ ì„¸ì…˜ ì„¤ì • ì‹¤íŒ¨: \(error.localizedDescription)")
            return
        }

        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        recognitionRequest.shouldReportPartialResults = true

        let inputNode = engine.inputNode
        let inputFormat = inputNode.inputFormat(forBus: 0)
        inputNode.removeTap(onBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { buffer, _ in
            self.recognitionRequest?.append(buffer)
        }

        // ì—”ì§„ ì‹œìž‘
        do {
            engine.prepare()
            try engine.start()
            print("ðŸŽ¤ AVAudioEngine ì‹œìž‘ë¨")
            startSilenceMonitor()
        } catch {
            print("âŒ AVAudioEngine ì‹œìž‘ ì‹¤íŒ¨: \(error.localizedDescription)")
            stopRecording()
            return
        }

        // ìŒì„± ì¸ì‹ ì‹œìž‘
        DispatchQueue.main.async {
            self.recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { result, error in
                if let result = result {
                    let text = result.bestTranscription.formattedString
                    DispatchQueue.main.async {
                        self.recognizedText = text
                        self.lastVoiceDetectedTime = CACurrentMediaTime()

                        if !self.isListening {
                            self.isListening = true
                            self.foxScene?.isEmotionListening = true
                            self.foxScene?.updateFoxForListeningState()
                        }
                    }

                    let emotion = self.analyzeEmotion(from: text)
                    if emotion != "neutral", let fox = self.foxScene {
                        fox.performAction(for: emotion)
                    }
                }

                if let error = error {
                    print("âŒ ì¸ì‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: \(error.localizedDescription)")
                    self.stopRecording()
                }
            }
        }
    }

    // MARK: - ë¬´ìŒ ëª¨ë‹ˆí„°ë§
    private func startSilenceMonitor() {
        silenceCheckTimer?.invalidate()
        silenceCheckTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { _ in
            let now = CACurrentMediaTime()
            if self.isListening && (now - self.lastVoiceDetectedTime > self.silenceTimeout) {
                print("ðŸ”‡ ë¬´ìŒ ì§€ì† ê°ì§€ â†’ ë“£ê¸° ì¢…ë£Œ")
                self.isListening = false
                DispatchQueue.main.async {
                    self.foxScene?.isEmotionListening = false
                    self.foxScene?.updateFoxForListeningState()
                    self.recognizedText = ""
                }
            }
        }
    }

    private func stopSilenceMonitor() {
        silenceCheckTimer?.invalidate()
        silenceCheckTimer = nil
    }

    // MARK: - ë…¹ìŒ ì¤‘ì§€
    func stopRecording() {
        isListening = false

        engine.inputNode.removeTap(onBus: 0)
        engine.stop()
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()

        stopSilenceMonitor()

        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            print("âš ï¸ ì„¸ì…˜ ë¹„í™œì„±í™” ì‹¤íŒ¨: \(error.localizedDescription)")
        }

        print("ðŸ›‘ ë…¹ìŒ ì¤‘ì§€ë¨")
    }

    // MARK: - í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„
    func analyzeEmotion(from text: String) -> String {
        let lowered = text.lowercased()

        let emotionKeywords: [(keyword: String, emotion: String)] = [
            ("ê¸°ë»", "happy"), ("ì¢‹ì•„", "happy"), ("í–‰ë³µ", "happy"),
            ("ìŠ¬í¼", "sad"), ("ìš°ìš¸", "sad"), ("ëˆˆë¬¼", "sad"),
            ("í™”ë‚˜", "angry"), ("ì§œì¦", "angry"), ("ë¶„ë…¸", "angry"),
            ("ë†€ë¼", "surprised"), ("í—‰", "surprised"), ("ì–´ë¨¸", "surprised")
        ]

        var latestEmotion: String? = nil
        var latestRangeLocation = -1

        for (keyword, emotion) in emotionKeywords {
            if let range = lowered.range(of: keyword, options: .backwards) {
                let location = lowered.distance(from: lowered.startIndex, to: range.lowerBound)
                if location > latestRangeLocation {
                    latestRangeLocation = location
                    latestEmotion = emotion
                }
            }
        }

        return latestEmotion ?? "neutral"
    }
}
